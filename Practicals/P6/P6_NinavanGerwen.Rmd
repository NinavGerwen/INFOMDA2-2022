---
title: "Practical 6: Model-based Clustering"
author: "Nina van Gerwen (1860852)"
date: "2022-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1: Introduction

First, we load the packages and data.

```{r}
library(mclust)
library(tidyverse)
library(patchwork)

df <- as_tibble(banknote)
```

## 2: Take home exercises

### 2.1: Data exploration

#### 2.1.1: Describing the dataset

The dataset contains six measurements made on 100 genuine and 100 counterfeit
old-Swiss 1000-frank notes. So the clusters are *genuine* and *counterfeit*, and
then all the features are information about the bill (e.g., length).

#### 2.1.2: Visualising the data

```{r}
df %>%
  ggplot(., aes(x = Left, y = Right, col = Status)) +
  geom_point() + theme_minimal() + geom_jitter(aes(col = Status))
```

Looking at the plot, I would say that there is some difference between the clusters
on the features Left and Right. however, there is still quite some overlap also.

#### 2.1.3: Removing the categorical variable

```{r}
df <- select(df, -Status)
```


#### 2.1.4: Creating a density plot for every variable/feature

```{r}
df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") + 
  geom_density() +
  theme_minimal()
```

From the above six plots, we find that especially the feature Diagonal might be
best for clustering.

### 2.2: Univariate model-based clustering

#### 2.2.5: Using mclust to perform model-based clustering

```{r}
set.seed(1248)
fit_E_2 <- Mclust(data = df$Diagonal, G = 2, modelNames = "E")

fit_E_2$parameters
```

Above, we find that the means of the clusters are 139.4 and 141.5. And 
their variances are .244.

#### 2.2.6: Computing the BIC

Using the formula:

$$ \text{BIC} = -2 l(\theta) + k \cdot log(n)$$ 

We can calculate that the BIC is:

```{r}
-2 * fit_E_2$loglik + 3 * log(200)
```
The BIC value that the Mclust function computed was -569.47. This means
that there is a slight difference (besides the fact that it is negative).
The 3 parameters I used were: 2 means and one variance. 

#### 2.2.7: Plotting the implied density

```{r}
plot(fit_E_2, what = "density") + rug(df$Diagonal, col = "red")
```

#### 2.2.8: Clustering again with unequal variances

```{r}
set.seed(1248)
fit_V_2 <- Mclust(data = df$Diagonal, G = 2, modelNames = "V")

fit_V_2$parameters
```

Now we have two means of 139.5 and 141.6 and two variances that are .359 and .150.

```{r}
plot(fit_V_2, what = "density") + rug(df$Diagonal, col = "red")
```

Plotting this unequal variances model, we see that indeed one of the peaks in
the bimodal distribution is more spreadout with a lower peak.

#### 2.2.9: The new model's parameters

This unequal variance model now has 4 parameters: 2 means and 2 variances.

#### 2.2.10: Calculating the deviances

```{r}
-2 * fit_E_2$loglik

-2 * fit_V_2$loglik
```

According to the deviance, the unequal variance model is better. 

#### 2.2.11: Calculating the BICs

```{r}
-2 * fit_E_2$loglik + 3 * log(200)

-2 * fit_V_2$loglik + 4 * log(200)
```
According to the BIC, the unequal variance model is still better. In other words,
letting the cluster have a varying variance might be worth it (as it only costs
one extra parameter).

## 3: Lab Exercises

### 3.1: Multivariate model-based clustering

#### 3.1.12: Using Mclust with all features

```{r}
set.seed(1248)
fit_MV <- Mclust(data = df, G = 1:9)

fit_MV$BIC

plot(fit_MV, what = "BIC")
```
According to the results, the model with the most optimal BIC is a VVE model with
3 clusters with a BIC of -1607.57. This means the model allows for varying
volumes and shapes for each cluster, but an equal orientation.

#### 3.1.13: The number of mean parameters of the VVE-3 model

The model has 3 clusters and 6 features are nicluded. This means that
there are 3 x 6 mean parameters in this model.

#### 3.1.14: Running a 2-component VVV model and visualising

```{r}
set.seed(1248)
fit_MV_2 <- Mclust(data = df, G = 2, modelNames = "VVV")

plot(fit_MV_2, what = "density")
```

Looking at the matrix of bivariate density plots, we find that the features that give
a good component separation are: *Diagonal*, *Top* and *Bottom*. The features
that seem to not provide a good component separation are: *Length*, *Left* and
*Right*.

#### 3.1.15: Plotting the estimated class assignments to size aesthetics

```{r}
df %>%
  mutate(Est_Class = as.factor(fit_MV_2$classification),
         Uncertainty = abs(fit_MV_2$uncertainty)) %>%
  ggplot(data = ., aes(x = Left, y = Right, col = Est_Class)) +
  geom_point(aes(size = Uncertainty)) + geom_jitter(aes(col = Est_Class)) + theme_minimal()
```

The uncertainty seems to be largest around the overlapping area, which is
to be expected. However, the scale of uncertainty is a bit weird so I would
not draw any coclusions from it.

### 3.2: Challenge assignment

#### 3.2.16: Loading the HDclassif package

```{r}
library(HDclassif)
```

#### 3.2.17: Running a high-dimensional data clustering

An axis is created for every cluster, first they start out around a similar location
(which is decided upon by random initialisation). After which they branch out to
separate locations in multiple iterations. After each time that the axes moves,
new cluster predictions are also made to further improve the axis locations.
In the end, you end up with cluster allocations according to the 4 most 
'separated' axis.












  


