---
title: 'Practical 2: Dimension Reduction I'
author: "Nina van Gerwen (1860852)"
date: "2022-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup}
library(dplyr)
library(magrittr)
library(ggplot2)
```

## 2: Take home exercises

### 2.1: SVD and Eigendecomposition

```{r}
C <- read.table("Data/Example1.dat") %>%
  as.matrix(.) %>%
  scale(., scale = FALSE)
```


#### 2.2: Calculating sample size and covariance matrix

```{r}
N <- dim(C)[1]
S <- t(C) %*% C / N
```


#### 2.3: Obtaining the Singular Value Decomposition

```{r}
SVD <- svd(x = C)
```


#### 2.4: Inspecting the SVD

```{r}
U <- SVD$u
D <- SVD$d
V <- SVD$v
U
D
V
```

Yes, the matrices are the same as on the slides!

#### 2.5: Calculating principal component scores

```{r}
lambda <- U * D
```


#### 2.6: Plotting the principal component scores

```{r}
lambda %>%
  as.data.frame(.) %>%
  ggplot(., aes(x = V1, y = V2)) +
  geom_point() +
  xlim(-18, 18) +
  ylim(-16, 16)
```


#### 2.7: Applying eigendecomposition

```{r}
EV <- eigen(S)
```


#### 2.8: Inspecting the eigenvalues

```{r}
sum(EV$values) == sum(diag(S))

sum((lambda[, 1] - mean(lambda[, 1]))^2) / 9
```

The sum of the eigenvalues are at least equal to the diagonal of the
covariance matrix. However, the variance of the principle components
seems to be somewhat different.

#### 2.9: Calculating percentage of total variance explained by each components

```{r}
Eig$values[1] / sum(Eig$values) * 100
Eig$values[2] / sum(Eig$values) * 100
```

The first component explains 97.5% of the variance, and the second component
explains the left-over 2.5%  of the variance.

### 2.2: Principle Component Analysis

First we have to obtain the sample correlation matrix.

```{r}
SCM <- matrix(data = c(1, 0.32, 0.95, 0.94, 0.84, 0.22, 0.47, 0.82, 0.32, 1, 0.06,
                       0.21, 0.01, 0.30, 0.10, 0.01, 0.95, 0.06, 1, 0.94, 0.89, 0.14,
                       0.44, 0.81, 0.94, 0.21, 0.94, 1, 0.88, 0.19, 0.50, 0.68,
                       0.84, 0.01, 0.89, 0.88, 1, -0.23, 0.55, 0.63, 0.22, 0.30,
                       0.14, 0.19, -0.23, 1, -0.15, 0.21, 0.47, 0.10, 0.44, 0.50,
                       0.55, -0.15, 1, 0.14, 0.82, 0.01, 0.81, 0.68, 0.63, 0.21,
                       0.14, 1), nrow = 8, ncol = 8, byrow = TRUE)
SCM
```

#### 2.9: Applying PCA to the SCM

```{r}
PCA <- eigen(SCM)
```

#### 2.10: Interpreting through the greater-than-one rule

```{r}
sum(PCA$values > 1)
```

Three principal components should be extracted according to this rule!

#### 2.11: Explained variance by the three components

```{r}
sum(PCA$values[1:3]) / sum(PCA$values) * 100
```

The first three principal components explain 88.9% of the total variance.

#### 2.12: Making a scree plot

```{r}
PCA$values %>%
  data.frame(EV = .) %>%
  ggplot(., aes(x = c(1:8), y = EV)) +
  geom_line() + theme_minimal()
```

According to the scree-plot, you would only extract two principal components!

#### 2.13: Explained variance by the first two principal components

```{r}
sum(PCA$values[1:2]) / sum(PCA$values) * 100
```

The first two principal components explain 76.3% of the total variance.

## 3: Lab Exercises




