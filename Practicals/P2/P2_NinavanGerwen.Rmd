---
title: 'Practical 2: Dimension Reduction I'
author: "Nina van Gerwen (1860852)"
date: "2022-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(dplyr)
library(magrittr)
library(ggplot2)
```

## 2: Take home exercises

### 2.1: SVD and Eigendecomposition

```{r}
C <- read.table("Data/Example1.dat") %>%
  as.matrix(.) %>%
  scale(., scale = FALSE)
```


#### 2.2: Calculating sample size and covariance matrix

```{r}
N <- dim(C)[1]
S <- t(C) %*% C / N
```


#### 2.3: Obtaining the Singular Value Decomposition

```{r}
SVD <- svd(x = C)
```


#### 2.4: Inspecting the SVD

```{r}
U <- SVD$u
D <- SVD$d
V <- SVD$v
U
D
V
```

Yes, the matrices are the same as on the slides!

#### 2.5: Calculating principal component scores

```{r}
lambda <- U * D
```


#### 2.6: Plotting the principal component scores

```{r}
lambda %>%
  as.data.frame(.) %>%
  ggplot(., aes(x = V1, y = V2)) +
  geom_point() +
  xlim(-18, 18) +
  ylim(-16, 16)
```


#### 2.7: Applying eigendecomposition

```{r}
EV <- eigen(S)
```


#### 2.8: Inspecting the eigenvalues

```{r}
sum(EV$values) == sum(diag(S))

sum((lambda[, 1] - mean(lambda[, 1]))^2) / 9
```

The sum of the eigenvalues are at least equal to the diagonal of the
covariance matrix. However, the variance of the principle components
seems to be somewhat different.

#### 2.9: Calculating percentage of total variance explained by each components

```{r}
EV$values[1] / sum(EV$values) * 100
EV$values[2] / sum(EV$values) * 100
```

The first component explains 97.5% of the variance, and the second component
explains the left-over 2.5%  of the variance.

### 2.2: Principle Component Analysis

First we have to obtain the sample correlation matrix.

```{r}
SCM <- matrix(data = c(1, 0.32, 0.95, 0.94, 0.84, 0.22, 0.47, 0.82, 0.32, 1, 0.06,
                       0.21, 0.01, 0.30, 0.10, 0.01, 0.95, 0.06, 1, 0.94, 0.89, 0.14,
                       0.44, 0.81, 0.94, 0.21, 0.94, 1, 0.88, 0.19, 0.50, 0.68,
                       0.84, 0.01, 0.89, 0.88, 1, -0.23, 0.55, 0.63, 0.22, 0.30,
                       0.14, 0.19, -0.23, 1, -0.15, 0.21, 0.47, 0.10, 0.44, 0.50,
                       0.55, -0.15, 1, 0.14, 0.82, 0.01, 0.81, 0.68, 0.63, 0.21,
                       0.14, 1), nrow = 8, ncol = 8, byrow = TRUE)
SCM
```

#### 2.9: Applying PCA to the SCM

```{r}
PCA <- eigen(SCM)
```

#### 2.10: Interpreting through the greater-than-one rule

```{r}
sum(PCA$values > 1)
```

Three principal components should be extracted according to this rule!

#### 2.11: Explained variance by the three components

```{r}
sum(PCA$values[1:3]) / sum(PCA$values) * 100
```

The first three principal components explain 88.9% of the total variance.

#### 2.12: Making a scree plot

```{r}
PCA$values %>%
  data.frame(EV = .) %>%
  ggplot(., aes(x = c(1:8), y = EV)) +
  geom_line() + theme_minimal()
```

According to the scree-plot, you would only extract one principal components!

#### 2.13: Explained variance by the first principal component

```{r}
sum(PCA$values[1]) / sum(PCA$values) * 100
```

The first principal component explains 58.2% of the total variance.

## 3: Lab Exercises

```{r}
mtcars <- mtcars
```


```{r}
mtcars.pca <- prcomp(mtcars[, c(1:7, 10, 11)],
                     center = TRUE,
                     scale = TRUE)
```


#### 3.14: Inspecting the PCA

```{r}
summary(mtcars.pca)
```


#### 3.15: Explained variances by PC1

The summary function showed that the proportion of variance explained
by PC1 is 62.8%.

#### 3.16.1: Cumulative explained variance by PC1, PC2 and PC3

The summary function showed that the cumulative proportion of variance
explained by the first three components is 91.6%.

#### 3.16.2: Determining the eigenvalues

```{r}
eigen(cov(scale(mtcars)))$values
```

According to the greater-than-one rule, we should
extract only the first two principal components.

#### 3.17: The total variance

Due to the fact that the data has been both centered and scaled, the
total variance is equal to *p*, the number of features (i.e., 11).

#### 3.18: Explained variance by P1 and P2

If we extract only the first two principal components, the cumulative
proportion of explained variance is 86%.

#### 3.19: Creating a biplot for the first two PC

```{r}
biplot(mtcars.pca, choices = c(1,2))
```

#### 3.20: Creating and interpreting a biplot between PC2 and PC3

```{r}
biplot(mtcars.pca, choices = c(1,3))
```

From this plot, we can see that the car brand that has exceptional positive values
on PC3, and negative positive values on PC1 is *Merc 230*.

#### 3.21: Creating and interpreting the screeplot 

```{r}
screeplot(mtcars.pca, type = "lines")
```

Interpreting this plot, I would argue the screeplot states that you should extract
either 1 or 2 principal components (because both the second and third component can be
seen as an 'elbow'). If you were to choose 2, this would be in agreement with the
eigenvalues-greater-than-one rule. Taking these results together, I would therefore
choose the first 2 PC.




