---
title: 'Practical 7: Time Series'
author: "Nina van Gerwen (1860852)"
date: "2023-01-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1: Introduction

```{r}
library(expsmooth)
library(fpp3)
library(fable.prophet)
```


## 2: Take-home exercises

### 2.1: Data exploration

#### 2.1.1: Describing the structure of the data

```{r}
data(ukcars)
ukcars %>% str()
```
The ukcars dataset is a time-series dataset from 1977 to 2005 with 113 values.
This means the data is probably seasonal/quarterly.

#### 2.1.2: Converting to tsibble

```{r}
ts_cars <- ukcars %>% as_tsibble(.)
```

ts_cars has a different structure. It is no longer a time-series. Instead, it is
a time-series tibble (i.e., a tibble with an index variable that denotes the time
of the observations).

### 2.1.3: Some line plot visualisation

```{r}
ts_cars %>% autoplot(value)
```

There seems to be a large seasonal effect and a somewhat increasing trend past
1980. 

#### 2.1.4: Further line plot visualisations

```{r}
ts_cars %>% filter(year(index) > 1980 & year(index) < 2000) %>%
  autoplot(value)
```

When looking only at the period between 1980 and 2000, the increasing trend
becomes even more obvious with the same seasonal effect as before.

### 2.1.5: Plotting the autocorrelation

```{r}
ts_cars %>% ACF(value) %>% autoplot(.)
```

Specific features to notice about this plot is the returning pattern every
4 lags. Furthermore, autocorrelation for every lag is significant.

## 3: Lab exercises

### 3.1: Decomposition

#### 3.1.6: Creating an STL decomposition

```{r}
set.seed(1248)
STL_decomp <- ts_cars %>% 
  model(STL(value ~ trend(window = 15) + season(window = Inf))) %>% 
  components() 

STL_decomp %>%
  head()

```


#### 3.1.7: Plotting the individual components

```{r}
STL_decomp %>% autoplot()
```

#### 3.1.8: Plotting the autocorrelation of the remainder

```{r}
STL_decomp %>% 
  select(remainder) %>% 
  ACF(remainder) %>% 
  autoplot(.)
```

According to the Box-Jenkins methodology, this could be slightly improved, yes.
Currently, 9 out of the 20 lags are still a significant correlation, which means
that the residuals are not yet completely 'white noise'.

### 3.2: ARIMA modeling 

#### 3.2.9: Creating two ARIMA models

```{r}
models <- 
  ts_cars %>%
  model(
    ARIMA = ARIMA(value ~ pdq(1, 1, 1) + PDQ(0, 0, 0)),
    SARIMA = ARIMA(value)
  )
```

The first ARIMA model is an Autoregressive Integrated Moving Average model, with
three parts: First, an autoregressive model with lag 1, this means that
value $y_t$ is autoregressed on only the first value before it (i.e., $y_{t-1}$). 
Second, we don't work with the actual values but the difference between the values (in order
to achieve stationarity). And third, the model also includes a Moving Average part. This means 
that value $y_t$ is also predicted by the the first error before the value 
(i.e., $y_t$ is dependent on $\epsilon_{t-1}$). Furthermore, there is *NO* seasonality
in this model (defined by the PDQ(0, 0, 0) part).

The second model, instead, is an automatic ARIMA specification, where it has
simply chosen the best model (i.e., lowest AIC) after testing multiple models.
The outcome for this model was a model with pdq(1,0,1) + PDQ(1,1,2).

#### 3.2.10: Creating forecasts

```{r}
models %>% 
  select(ARIMA) %>% 
  forecast(., h = "5 years") %>%
  autoplot(.)

models %>%
  select(SARIMA) %>%
  forecast(., h = "5 years") %>%
  autoplot(.)
```

The main difference is obviously the seasonsality, which the SARIMA model
accounts for, whereas the ARIMA model does not. Furthermore, the SARIMA
model seems to have smaller intervals (which is to be expected as it was the
model with the lowest AIC). However, this might be due to overfitting.

An obvious similarity between both plots is that for both plots, the variance
of the point forecasts increases with time, as it should.

### 3.3: Comparing SARIMA and Prophet forecasting methods

#### 3.3.11. Fitting a data-driven ARIMA and a prophet model

```{r}
ARIMA_FC <- ts_cars %>% 
  filter(year(index) > 1980 & year(index) < 1995) %>%
  model(ARIMA(value)) %>%
  forecast(., h = "5 years")

PROPH_FC <- ts_cars %>% 
  filter(year(index) > 1980 & year(index) < 1995) %>%
  model(prophet(value)) %>%
  forecast(., h = "5 years")

MSE(.resid = ARIMA_FC$.mean, .actual = filter(ts_cars, year(index) > 1995 & year(index) < 2000))


MSE(.resid = PROPH_FC$.mean, .actual = filter(ts_cars, year(index) > 1995 & year(index) < 2000))
```

According to the mean squared error, the prophet model has better predictions.





