---
title: 'Practical 8: Text Mining'
author: "Nina van Gerwen (1860852)"
date: "2023-01-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1: Introduction

```{r}
library(tidyverse)
library(tidytext)
library(tm)
library(e1071)
library(topicmodels)
library(stringi)
library(magrittr)
```

## 2: Vector Space Model: Document-Term Matrix

### 2.1: Loading the data

```{r}
load("data/news_dataset.rda")
head(df_final)
```

### 2.2: Finding out the categories and number of observations

```{r}
df_final %>% 
  mutate(Category = as.factor(Category)) %>%
  select(Category) %>% 
  summary(.)
```

### 2.3: Converting into a Document-Term Matrix

```{r}
## To convert it into a DTM, first we preprocesses it into a corpus data
## while also changing certain things
corpusdata <- df_final %$%
  ## Select the content
  Content %>% 
  ## Vectorsource and corpus it
  VectorSource() %>% 
  Corpus() %>% 
  ## Then, through iconv, get rid of all non-UTF8 characters
  tm_map(iconv, from="UTF-8", to="UTF-8", sub="") %>% 
  ## And lowercase everything
  tm_map(content_transformer(tolower)) %>% 
  ## Remove the stopwords
  tm_map(removeWords, stopwords()) %>% 
  ## Strip any excess white space
  tm_map(stripWhitespace) %>% 
  ## Remove any punctiation
  tm_map(removePunctuation) %>% 
  ## And finally remove any numbers
  tm_map(removeNumbers)

## Then we make it a DTM
dtm <- DocumentTermMatrix(corpusdata)

## And we can find the terms that have a frequency higher than 10 like this:
frequent_terms <- findFreqTerms(dtm,lowfreq = 10,highfreq = Inf)
```

### 2.4: Splitting the data into training and test

```{r}
## To split the dataset, we sample from the indexes of the
## original dataframe and state that we want 80% of the indexes sampled
train_index <- sample(nrow(df_final),floor(0.8*nrow(df_final)))

# And then the sampled indexes will be the training set
train_data <- df_final[train_index,]
## And the leftover indexes will be the test set
test_data <- df_final[-train_index,]

## And we do the same for the documents in the corpus data stuff
train_corpus <- corpusdata[train_index]
test_corpus <- corpusdata[-train_index]
```


### 2.5: Creating separate DTMs for the training and test set and converting them into dataframes

```{r}
## Select the train corpus
train_dtm <- train_corpus %>%
  ## Turn it into a DTM with only the frequent terms
  DocumentTermMatrix(list(dictionary=frequent_terms)) %>% 
  ## Then as matrix
  as.matrix() %>%
  ## Then as data frame
  as.data.frame()

## Do the same for test corpus
test_dtm <- test_corpus %>% 
  DocumentTermMatrix(list(dictionary=frequent_terms)) %>% 
  as.matrix() %>% 
  as.data.frame()
```


## 3: Topic modeling

### 3.8: Applying the LDA function

```{r}
set.seed(1248)

LDA_model <- LDA(x = train_dtm, k = 5, method = "Gibbs")
```


### 3.9: Extracting per-topic-word-probabilities

```{r}
library(reshape2)
probs_per_topic <- tidy(LDA_model, matrix = "beta")
```

### 3.10: Plotting top 20 terms per topic

```{r}
lda_top_terms <- probs_per_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 20) %>% # We use dplyrâ€™s slice_max() to find the top 10 terms within each topic.
  ungroup() %>%
  arrange(topic, -beta)

lda_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```


### 3.11: Restructuring the terms and topic data in a wide format

```{r}
beta_wide <- probs_per_topic %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  mutate(log_ratio21 = log2(topic2 / topic1)) %>% 
  mutate(log_ratio31 = log2(topic3 / topic1))%>% 
  mutate(log_ratio41 = log2(topic4 / topic1))%>% 
  mutate(log_ratio51 = log2(topic5 / topic1))
```

### 3.12: Visualising greatest log ratio differences

```{r}
# topic 1 versus topic 2
lda_top_terms1 <- beta_wide %>%
  slice_max(log_ratio21, n = 10) %>%
  arrange(term, -log_ratio21)

lda_top_terms2 <- beta_wide %>%
  slice_max(-log_ratio21, n = 10) %>%
  arrange(term, -log_ratio21)

lda_top_terms12 <- rbind(lda_top_terms1, lda_top_terms2)

# this is for ggplot to understand in which order to plot name on the x axis.
lda_top_terms12$term <- factor(lda_top_terms12$term, levels = lda_top_terms12$term[order(lda_top_terms12$log_ratio21)])

# Words with the greatest difference in beta between topic 2 and topic 1
lda_top_terms12 %>%
  ggplot(aes(log_ratio21, term, fill = (log_ratio21 > 0))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  theme_minimal()
```

### 3.13: Getting the per-document-per-topic probabilities

```{r}
lda_documents <- tidy(LDA_model, matrix = "gamma")
```


### 3.14: Examining topic probabilities for certain books

```{r}
lda_documents %>%
  filter(document == c(1, 1000, 2000, 2225))
```

### 3.15: Visualising topic probabilities for these documents

```{r}
# reorder titles in order of topic 1, topic 2, etc before plotting
lda_documents[lda_documents$document %in% c(1, 1000, 2000, 2225),] %>%
  mutate(document = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document) +
  labs(x = "topic", y = expression(gamma)) +
  theme_minimal()
```





