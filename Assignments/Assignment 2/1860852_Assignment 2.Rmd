---
title: "Assignment 2: Comparing cluster methods"
author: "Nina van Gerwen (1860852)"
date: "2023-01-14"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Comparing cluster methods

### Describing the dataset

For my dataset, I chose the 'Estimation of obesity levels based on
eating habits and condition' data, gained from the [UCI Machine Learning
Repository](https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+#).
The raw data contains a total of 2111 observations on 17 variables. The
data was gained from individuals in Mexico, Peru and Colombia. The
variables include demographic information (e.g., gender, age), variables
related to their food intake (e.g., FAVC: frequent consumption of high
caloric food, FCVC: frequency of consumption of vegetables, NCP: number
of main meals, etc.) and finally variables related to physical condition
(e.g., SCC: calories consumption monitoring, FAF: physical activity
frequency, etc.). A full description of all variables can be found
[here](https://www-sciencedirect-com.proxy.library.uu.nl/science/article/pii/S2352340919306985?via%3Dihub).The
goal of this dataset will be to properly cluster different weight
categories according to their food habits and physical activity, as
coded in the variable NObeyesdad. Important to note is that only 23% of
the data was collected, whilst 77% of the data was generated
synthetically. Furthermore, the dataset contains no missing information.

```{r}
## Load tidyverse for tidying later
library(tidyverse)

## Load the raw data
raw_data <- read.csv("raw_data/Raw_Obesity_Data.csv")

## Look at the structure of the data
str(raw_data)
```

### Preprocessing the data

```{r}
## To process the data, we only change all
## variables to their appropriate class through mutate
processed_data <- raw_data %>%
  mutate(Gender = as.factor(Gender),
         Age = as.integer(round(Age)),
         ## Round weight to one decimal
         Weight = round(Weight, digits = 1),
         ## Turn height in cm
         Height = round(100 * Height),
         ## Change name
         Family_Occ = as.factor(family_history_with_overweight),
         FAVC = as.factor(FAVC),
         ## Note, for FCVC, NCP, CH2O, FAF and TUE we first make them
         ## integers to round them and then make them factors
         ## because otherwise they become factors with 100+ levels
         FCVC = as.factor(as.integer(FCVC)),
         NCP = as.factor(as.integer(NCP)),
         CAEC = as.factor(CAEC),
         SMOKE = as.factor(SMOKE),
         CH2O = as.factor(as.integer(CH2O)),
         SCC = as.factor(SCC),
         FAF = as.factor(as.integer(FAF)),
         TUE = as.factor(as.integer(TUE)),
         CALC = as.factor(CALC),
         MTRANS = as.factor(MTRANS),
         NObeyesdad = as.factor(NObeyesdad)) %>%
  ## We remove the variable with the long name that we had turned into
  ## a more readable name
  select(-family_history_with_overweight) %>%
  ## Finally, we standardise/scale the numerical values also
  mutate(Age = (Age - mean(Age)) / sd(Age),
         Weight = (Weight - mean(Weight)) / sd(Weight),
         Height = (Height - mean(Height)) / sd(Height))

## And we save the processed data as a RDS file
saveRDS(processed_data, file = "processed_data/processed_data.RDS")
```

Because the number of variables is not too large (only 16 excluding the
outcome variable) and all variables seem to be theoretically relevant
(e.g., for FAF: not having any physical activity can be an indicator of
having a higher weight), I opted to keep all variables included. Note
that this does not necessarily mean that all variableswill be used in
the clustering models.

### Two clustering methods

To make an informed decision on which models might work, let us first
look at some descriptive statistics of the data (not including the
outcome variable, as that would be cheating) and visualizations.

```{r}
## Load patchwork package for plotting
library(patchwork)

processed_data %>% 
  select(-NObeyesdad) %>%
  summary(.)

vars <- processed_data %>% 
  select_if(., is.numeric) %>% 
  colnames(.) %>%
  as.factor(.)

par(mfrow = c(1,3))
for(levels in vars){
  processed_data %>%
    select(all_of(levels)) %>%
    as.matrix(.) %>%
    as.numeric(.) %>%
    hist(., main = as.character(levels))
}

hist_one <- 
  ggplot(data = processed_data, aes(x = Weight)) +
  geom_histogram(aes(y = after_stat(density)), bins = 39) +
  facet_wrap(facets = "MTRANS")

hist_two <- 
  ggplot(data = processed_data, aes(x = Weight)) +
  geom_histogram(aes(y = after_stat(density)), bins = 39) +
  facet_wrap(facets = "FAF")

hist_three <-
  ggplot(data = processed_data, aes(x = Height)) +
  geom_histogram(aes(y = after_stat(density)), bins = 39) +
  facet_wrap(facets = "CAEC")

hist_four <- 
  ggplot(data = processed_data, aes(x = Height)) +
  geom_histogram(aes(y = after_stat(density)), bins = 39) +
  facet_wrap(facets = "SCC")

hist_one + hist_two + hist_three + hist_four + plot_layout(2, 2)

plot_one <- ggplot(processed_data, aes(x = Weight, y = Height, col = as.factor(CH2O))) +
  geom_point() + theme(legend.position = "none")

plot_two <- ggplot(processed_data, aes(x = Weight, y = Height, col = as.factor(FCVC))) +
  geom_point() + theme(legend.position = "none")

plot_three <- ggplot(processed_data, aes(x = Weight, y = Height, col = as.factor(SCC))) +
  geom_point() + theme(legend.position = "none")

plot_four <- ggplot(processed_data, aes(x = Weight, y = Height, col = as.factor(FAF))) +
  geom_point() + theme(legend.position = "none")

plot_five <- ggplot(processed_data, aes(x = Weight, y = Height, col = Gender)) +
  geom_point() + theme(legend.position = "none")

plot_six <- ggplot(processed_data, aes(x = Weight, y = Height, col = CALC)) +
  geom_point() + theme(legend.position = "none")

plot_seven <- ggplot(processed_data, aes(x = Weight, y = Height, col = MTRANS)) +
  geom_point() + theme(legend.position = "none")

plot_eight <- ggplot(processed_data, aes(x = Weight, y = Height, col = Family_Occ)) +
  geom_point() + theme(legend.position = "none")

plot_one + plot_two + plot_three + plot_four + plot_five + plot_six +
  plot_seven + plot_eight + plot_layout(ncol = 2, nrow = 4)
```

Looking at the histograms, visual inspection shows that the numeric
variables, height and weight seem to be normally distributed. Looking at
their relative frequency given certain categories of variables, they
still seem to be normally distributed. Therefore, I feel safe to
generalise these findings to assume that the data within each cluster
might be multivariate normally distributed. This means that Gaussian
Mixture Models (GMM) might work well.

Furthermore, I created eight scatterplots with Weight on the x-axis and
Height on the y-axis. The plots show that the clusters do not seem to be
circular, but instead elliptical, in the space of data. This is another
argument in favor of GMM.

Therefore, I decided that GMM will be the first method. As for the
second method, I chose for Hierarchical clustering. This is because,
although we could easily use K-means clustering, as we know the number
of categories in the outcome variable, I am curious whether hierarchical
clustering will be able to properly show the number of correct
categories in a dendrogram. For the Hierarchical clustering, I chose
complete linkage in order to get a more balanced dendrogram with
Euclidean distances.

To summarize, the two clustering methods I chose for the current dataset
are:

-   Gaussian Mixture Modeling

-   Agglomerative Hierarchical Clustering with euclidean distances and
    complete linkage

### Applying the two methods

```{r}
set.seed(1248)

## First, make sure we remove the true categories variable
train_set <- processed_data %>%
  select(-NObeyesdad)

## Then, for GMM:
library(mclust)

## Fit the model with 1 to 9 clusters
GMM_fit <- Mclust(train_set, G = 1:9, verbose = FALSE)

## Check which model has the best BIC
plot(GMM_fit, what = "BIC")

GMM_fit$BIC

## Hierarchical clustering:
library(ggdendro)
distances <- dist(train_set, method = "euclidean")
hclust_res <- hclust(distances, method = "complete")

ggdendrogram(hclust_res) + coord_flip()
```

The results of the two clustering method shows that the best fitting GMM
are ones that allow for a varying shape (as all of the top three models
start with a V). This is what we expected, due to the elliptical shape
of the above plotted graphs. As for the hierarchical clustering method,
we find that on the horizontal axis, the cut off for 7 clusters does not
seem to be very far removed from 6. This might indicate that the method
perhaps has issues discriminating between some clusters (for example
between Overweight type I and II). Furthermore, due to the
semi-categorical nature of many variables, euclidean distances might not
have been the best choice.

### Evaluating and comparing the two methods

To evaluate and compare the two methods, I will compare them on three
factors, namely:

1.  Cluster stability, as measured by computing 100 bootstrap samples

2.  Internal indices, more specifically by computing the silhouette
    coefficient for the two methods

3.  External validity, by investigating whether the predicted categories
    from the two clustering methods are associated with their true
    category. Whether they are associated will be investigated through a
    $\chi^2$-test of independence and the correlation coefficient,
    calculated through the Spearman-Brown formula.

Below, you find the implementation for all three factors for both
models.

```{r}
## Stability
library(fpc)

## For the hclust method
hclust_boot <- train_set %>% 
  ## First, we have to recode all factors back to numerics, otherwise
  ## the method does not work
  mutate(Gender = as.numeric(Gender) - 1,
         FAVC = as.numeric(FAVC) - 1,
         FCVC = as.numeric(FCVC) - 1,
         NCP = as.numeric(NCP) - 1,
         CAEC = as.numeric(CAEC) - 1,
         SMOKE = as.numeric(SMOKE) - 1,
         CH2O = as.numeric(CH2O) - 1,
         SCC = as.numeric(SCC) - 1,
         FAF = as.numeric(FAF) - 1,
         TUE = as.numeric(TUE) - 1,
         CALC = as.numeric(CALC) - 1, 
         MTRANS = as.numeric(MTRANS) - 1,
         Family_Occ = as.numeric(Family_Occ) - 1) %>%
  ## Then we specify the method we want the stability for
  clusterboot(data = ., B = 100, k = 7, count = FALSE,
              clustermethod = hclustCBI,
              method = "complete", cutree = "7")

## And we check the stability of the 7 clusters
hclust_boot$bootmean

## And we do the same for the GMM clustering method
mclust_boot <- train_set %>% 
  mutate(Gender = as.numeric(Gender) - 1,
         FAVC = as.numeric(FAVC) - 1,
         FCVC = as.numeric(FCVC) - 1,
         NCP = as.numeric(NCP) - 1,
         CAEC = as.numeric(CAEC) - 1,
         SMOKE = as.numeric(SMOKE) - 1,
         CH2O = as.numeric(CH2O) - 1,
         SCC = as.numeric(SCC) - 1,
         FAF = as.numeric(FAF) - 1,
         TUE = as.numeric(TUE) - 1,
         CALC = as.numeric(CALC) - 1, 
         MTRANS = as.numeric(MTRANS) - 1,
         Family_Occ = as.numeric(Family_Occ) - 1) %>%
  clusterboot(data = ., B = 100, count = FALSE, k = 7,
              clustermethod = noisemclustCBI, G = 7, 
              multipleboot = FALSE, verbose = FALSE)

mclust_boot$bootmean
```

```{r}
## Silhouette (Internal indices)
library(cluster)

## Through silhouette function, we can easily get this for:

## The hierarchical clustering method:
silhouette(x = (cutree(hclust_res, 7)), dist = distances) %>% 
  summary(.)

## The GMM clustering method:
silhouette(x = GMM_fit$classification, dist = distances) %>%
  summary(.)

```

```{r}
## External validity

## We create a contingency table between the predicted and true classes
## for the hierarchical clustering method
table(true = processed_data$NObeyesdad, 
      hclust = as.factor(cutree(hclust_res, 7))) %>%
  ## And check whether they are associated wwith one another
  chisq.test(.)

## The same is done for the GMM clustering method
table(true = processed_data$NObeyesdad, mclust = GMM_fit$classification) %>%
  chisq.test(.)

## Besides this, we also get the Spearman-Brown correlation coefficient
## between the predicted and true categories

## For both the hierarchical clustering method
cbind(processed_data$NObeyesdad, as.factor(cutree(hclust_res, 7))) %>%
  cor(x = ., method = "spearman")

## And the GMM clustering method
cbind(processed_data$NObeyesdad, GMM_fit$classification) %>%
  cor(x = ., method = "spearman")
```

The three comparisons show that the Gaussian Mixture Model has: (a) a
higher average stability for the Gaussian Mixture Model, (b) higher
values on the average silhouette per cluster and (c) a higher external
validity correlation value. Although the $\chi^2$-tests of independence
was significant for both models, this might be due to a large sample
size. Because looking at the correlation, we find that the clusters made
by the Hierarchical Clustering method has a correlation of only 0.07
with the true clusters. The Gaussian Mixture Model's predicted clusters
instead had a correlation of 0.31. However, I would argue that the
external validity for the Gaussian Mixture Model is still somewhat low.

## Conclusion

Looking back at the two different clustering methods, I think a few
things become clear. First and foremost, we can safely state that a
Gaussian Mixture Model with elliptical shapes was able to outperform an
agglomerative hierarchical clustering method, as shown by the comparison
above. However, there are still many issues to be had with the Gaussian
Mixture modelling. For example, many of the Gaussian Mixture Models were
not able to converge (as seen by the NAs in the output). Furthermore,
many of the variables that we used for the clustering method were of a
categorical nature (or atleast, not quite on an interval measurement
level). And for both models, this might have lead to issues. For the
agglomerative clustering method, euclidean distances might not have been
the best choice for example. Furthermore, since the variables are not
completely on an interval measurement level, does it make sense for us
to assume they follow a normal distribution? Perhaps it would have been
better to compute a 'total score' made up by multiple of the categorical
variables and use this instead. To summarise, although we were able to
achieve some succes in our clustering methods, there were still many
limitations we have to keep in mind and further research would be
necessary in order to improve the predicted clusters.
